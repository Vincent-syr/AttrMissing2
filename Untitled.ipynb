{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T02:20:09.391066Z",
     "start_time": "2020-08-17T02:20:07.720318Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import json\n",
    "import torch.utils.data.sampler\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "import configs\n",
    "import backbone\n",
    "import data.feature_loader as feat_loader\n",
    "from data.datamgr import SetDataManager\n",
    "from methods.protonet import ProtoNet\n",
    "from io_utils import model_dict, parse_args, get_resume_file, get_best_file , get_assigned_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T04:35:47.588930Z",
     "start_time": "2020-08-16T04:35:47.578418Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset'     , default='CUB',        help='CUB/miniImagenet/cross/omniglot/cross_char')\n",
    "parser.add_argument('--model'       , default='ResNet10',      help='model: Conv{4|6} / ResNet{10|18|34|50|101}') # 50 and 101 are not used in the paper\n",
    "parser.add_argument('--method'      , default='protonet',   help='baseline/baseline++/protonet/matchingnet/relationnet{_softmax}/maml{_approx}') #relationnet_softmax replace L2 norm with softmax to expedite training, maml_approx use first-order approximation in the gradient for efficiency\n",
    "parser.add_argument('--train_n_way' , default=5, type=int,  help='class num to classify for training') #baseline and baseline++ would ignore this parameter\n",
    "# parser.add_argument('--train_n_way' , default=30, type=int,  help='class num to classify for training') #baseline and baseline++ would ignore this parameter\n",
    "\n",
    "parser.add_argument('--test_n_way'  , default=5, type=int,  help='class num to classify for testing (validation) ') #baseline and baseline++ only use this parameter in finetuning\n",
    "\n",
    "parser.add_argument('--n_shot'      , default=5, type=int,  help='number of labeled data in each class, same as n_support') #baseline and baseline++ only use this parameter in finetuning\n",
    "parser.add_argument('--train_aug'   , default=True, action='store_true',  help='perform data augmentation or not during training ') #still required for save_features.py and test.py to find the model path correctly\n",
    "\n",
    "parser.add_argument('--split'       , default='novel', help='base/val/novel') #default novel, but you can also test base/val class accuracy if you want \n",
    "parser.add_argument('--save_iter', default=-1, type=int,help ='saved feature from the model trained in x epoch, use the best model if x is -1')\n",
    "parser.add_argument('--adaptation'  , action='store_true', help='further adaptation in test time or not')\n",
    "params = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T04:36:05.028873Z",
     "start_time": "2020-08-16T04:36:01.139915Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_all = []\n",
    "split_list = ['base', 'val']\n",
    "iter_num = 600\n",
    "\n",
    "few_shot_params = dict(n_way = params.test_n_way , n_support = params.n_shot)   # 5 way, 5 shot\n",
    "\n",
    "model           = ProtoNet( model_dict[params.model], **few_shot_params )\n",
    "model = model.cuda()\n",
    "checkpoint_dir = '%s/checkpoints/%s/%s_%s' %(configs.save_dir, params.dataset, params.model, params.method)\n",
    "if params.train_aug:\n",
    "    checkpoint_dir += '_aug'\n",
    "if not params.method in ['baseline', 'baseline++'] :\n",
    "    checkpoint_dir += '_%dway_%dshot' %( params.train_n_way, params.n_shot)\n",
    "\n",
    "# load  pre-trained-model params\n",
    "modelfile   = get_best_file(checkpoint_dir)\n",
    "tmp = torch.load(modelfile)\n",
    "model.load_state_dict(tmp['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T04:39:34.885870Z",
     "start_time": "2020-08-16T04:39:34.748260Z"
    }
   },
   "outputs": [],
   "source": [
    "novel_file = os.path.join( checkpoint_dir.replace(\"checkpoints\",\"features\"),\n",
    "                          \"base_best.hdf5\") #defaut split = novel, but you can also test base or val classes\n",
    "cl_data_file = feat_loader.init_loader(novel_file)\n",
    "n_way = 5, \n",
    "n_support = 5, \n",
    "n_query = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T07:48:48.047412Z",
     "start_time": "2020-08-16T07:48:47.937597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['all_feats', 'all_labels', 'count']>\n",
      "(5885, 512)\n",
      "(5885,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch041-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "file = h5py.File(novel_file)   # <KeysViewHDF5 ['all_feats', 'all_labels', 'count']>\n",
    "print(file.keys())\n",
    "feats = file['all_feats']\n",
    "# feats.shape               # 5888, 512\n",
    "labels = file['all_labels']\n",
    "# labels.shape     # (5888,)\n",
    "while np.sum(feats[-1]) == 0:\n",
    "    feats  = np.delete(feats,-1,axis = 0)\n",
    "    labels = np.delete(labels,-1,axis = 0)\n",
    "\n",
    "print(feats.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_h = nn.Sequential(\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch041-gpu",
   "language": "python",
   "name": "torch041-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "700px",
    "left": "1547px",
    "right": "20px",
    "top": "118px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
